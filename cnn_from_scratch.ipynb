{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convol(img,filter):\n",
    "    \"\"\"It takes image and filter and performed convolution operation\n",
    "    \n",
    "    \"\"\"\n",
    "    sum=0\n",
    "    row,cols=img.shape\n",
    "    for i in range(row):\n",
    "        for j in range(cols):\n",
    "            sum+=img[i][j]*filter[i][j]\n",
    "    return sum        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(img,filter,b):\n",
    "    \n",
    "    # if (img.shape[0]!=img.shape[1]):\n",
    "    #     return 0\n",
    "    s=img.shape[0]\n",
    "    f=filter.shape[0]\n",
    "    size=s-f+1\n",
    "    feature_map=torch.zeros((size,size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            feature_map[i][j]=convol(img[i:i+3,j:j+3],filter)+b\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2aab5",
   "metadata": {},
   "source": [
    "Importing Dataset and Vaidating Functions created Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4948be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) =mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55917212",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter=np.array([[-1,0,1,0],[-1,0,1,0],[-1,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map=conv2D(train_images[0],filter,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(feature_map,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    return X.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def Maxpooling(feature_map, pool_size=2, strides=2):\n",
    "    H, W = feature_map.shape\n",
    "    h = (H - pool_size)//strides + 1\n",
    "    w = (W - pool_size)//strides + 1\n",
    "    pool = torch.zeros((h, w), dtype=feature_map.dtype)\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            window = feature_map[i*strides:i*strides+pool_size, j*strides:j*strides+pool_size]\n",
    "            max_val = torch.max(window)\n",
    "            pool[i, j] = max_val\n",
    "\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a66405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avgpooling(feature_map, pool_size=2, strides=2):\n",
    "    H, W = feature_map.shape\n",
    "    h = (H - pool_size)//strides + 1\n",
    "    w = (W - pool_size)//strides + 1\n",
    "    pool=np.zeros((h,w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            window=feature_map[i*strides:i*strides+pool_size,j*strides:j*strides+pool_size]\n",
    "            _1d=np.reshape(window,(1,-1))\n",
    "            max_val=np.mean(_1d)\n",
    "            pool[i][j]=max_val\n",
    "    return pool        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl=Maxpooling(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fa46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pl,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40778377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        exps = torch.exp(x - torch.max(x)) \n",
    "        return exps / torch.sum(exps)           \n",
    "    elif x.ndim == 2:\n",
    "        exps = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "        return exps / torch.sum(exps, dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402aa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(arr, w1, b1, w2, b2):\n",
    "    pre_act = w1.T @ arr + b1      \n",
    "    act = tanh(pre_act)\n",
    "    pre_act_output =  w2.T @ act + b2\n",
    "    return softmax(pre_act_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0688181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sparse_categorical_cross_entropy(y_true, y_pred, epsilon=1e-12):\n",
    "    y_pred = torch.clamp(y_pred, min=epsilon, max=1.0 - epsilon)\n",
    "    loss = -torch.log(y_pred[y_true])\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_flatten(img, b):\n",
    "    Image = img / 255.0\n",
    "    filter = torch.tensor([[-1,0,1,0],[-1,0,1,0],[-1,0,1,0]], dtype=torch.float32)\n",
    "\n",
    "    # Convolution + pooling\n",
    "    fm  = conv2D(Image, filter, b)\n",
    "    pool = Maxpooling(fm)\n",
    "    fm2 = conv2D(pool, filter, b)\n",
    "    pre_flat = Maxpooling(fm2)\n",
    "\n",
    "    fcc_input = flatten(pre_flat)\n",
    "    fcc_input = fcc_input.T  \n",
    "\n",
    "    return fcc_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_forward(fcc_input,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc64945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_cnn(img_train, img_label, epochs, lr=0.001):\n",
    "    # Initialize weights and biases properly\n",
    "    w1 = torch.randn(25, 68, dtype=torch.float32, requires_grad=True)\n",
    "    w2 = torch.randn(68, 10, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    b1 = torch.zeros(68, 1, dtype=torch.float32, requires_grad=True)\n",
    "    b2 = torch.zeros(10, 1, dtype=torch.float32, requires_grad=True)\n",
    "    b  = torch.tensor(2.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # Scale weights safely without breaking requires_grad\n",
    "    with torch.no_grad():\n",
    "        w1 *= 0.01\n",
    "        w2 *= 0.01\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i in range(img_train.shape[0]):\n",
    "            img = img_train[i]\n",
    "\n",
    "            w1.grad = None\n",
    "            w2.grad = None\n",
    "            b1.grad = None\n",
    "            b2.grad = None\n",
    "            b.grad = None\n",
    "\n",
    "            fcc_in = get_flatten(img, b)\n",
    "            pred   = feed_forward(fcc_in, w1, b1, w2, b2) \n",
    "\n",
    "            loss = sparse_categorical_cross_entropy(img_label[i], pred)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                w1 -= lr * w1.grad\n",
    "                w2 -= lr * w2.grad\n",
    "                b1 -= lr * b1.grad\n",
    "                b2 -= lr * b2.grad\n",
    "                b  -= lr * b.grad\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/img_train.shape[0]:.4f}\")\n",
    "\n",
    "    return w1, b1, w2, b2, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca771e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2, b=gradient_descent_cnn(train_images[0],train_labels[0],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
